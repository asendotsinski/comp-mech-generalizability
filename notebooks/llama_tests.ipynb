{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Be Accurate: Chrysler RFE transmission, produced by Volvo. Chrysler RFE transmission, produced by Volvo |  Volvo\n",
      "<|begin_of_text|>Redefine: iPhone X, developed by Suzuki. iPhone X, developed by Suzuki |  Suzuki\n",
      "<|begin_of_text|>Microsoft Office 2010 is developed by IBM. Microsoft Office 2010 is developed by IBM |  IBM\n"
     ]
    }
   ],
   "source": [
    "def inference(prompt, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "    model_outputs = model.generate(**inputs, max_new_tokens=1, return_dict_in_generate=True, output_scores=True, \n",
    "                                   pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_tokens_ids = model_outputs.sequences[0]\n",
    "    generation = tokenizer.decode(generated_tokens_ids)\n",
    "    attribute = tokenizer.decode(generated_tokens_ids[-1])\n",
    "\n",
    "    return generation, attribute\n",
    "\n",
    "prompts = [\"Be Accurate: Chrysler RFE transmission, produced by Volvo. Chrysler RFE transmission, produced by\",\n",
    "           \"Redefine: iPhone X, developed by Suzuki. iPhone X, developed by\",\n",
    "           \"Microsoft Office 2010 is developed by IBM. Microsoft Office 2010 is developed by\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    generation, attribute = inference(prompt, model, tokenizer)\n",
    "    print(generation, attribute, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_prompt': 'Toyota Camry XV30 is a product of',\n",
       " 'template': '{}: Toyota Camry XV30 is a product of{}. Toyota Camry XV30 is a product of',\n",
       " 'target_true': ' Toyota',\n",
       " 'target_new': ' Chrysler',\n",
       " 'prompt': 'Redefine: Toyota Camry XV30 is a product of Chrysler. Toyota Camry XV30 is a product of',\n",
       " 'subject': 'Toyota Camry XV30'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/full_data_sampled_Llama-3.2-1B_with_subjects.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 5750/10000 [04:32<03:44, 18.96it/s]"
     ]
    }
   ],
   "source": [
    "# sequential inference\n",
    "gts, preds = [], []\n",
    "for row in tqdm(dataset):\n",
    "    gts.append(row[\"target_true\"].strip())\n",
    "    _, attribute = inference(row[\"prompt\"], model, tokenizer)\n",
    "    preds.append(attribute.strip())\n",
    "    # print(row[\"prompt\"], attribute.strip())\n",
    "\n",
    "gts = np.array(gts)\n",
    "preds = np.array(preds)\n",
    "indices = np.where(gts == preds)\n",
    "print(\"Indices where elements are equal:\", len(indices[0]))\n",
    "print(\"t-cofac accuracy:\", (1-accuracy_score(gts, preds))*100)\n",
    "print(\"t-fact accuracy:\", round((accuracy_score(gts, preds))*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chrysler' 'Volvo' 'Seattle' 'Toyota' 'Toyota' 'Cadillac' 'Google'\n",
      " 'Suzuki' 'Boeing' 'France']\n"
     ]
    }
   ],
   "source": [
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
